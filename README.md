# Transformer-attention
The full derivation of [Transformer](https://arxiv.org/abs/1706.03762) gradient.
compare the theory attention gradient with PyTorch attention gradient

- If you want see the detail calcualtion,please see [CN](https://zhuanlan.zhihu.com/p/562061005),[EN](https://say-hello2y.github.io/2022-09-07/attention-gradient)
  
