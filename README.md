# Transformer-attention
compare the theory attention gradient with PyTorch attention gradient
